{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOVYTzBIEzwI/lFZ/Yb/+py"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook builds on the work in the functional approach notebook and tries to develop the algorithm to match the calculation done for SGD with neuro-evolution. This notebook also uses the IRIS flowers dataset."],"metadata":{"id":"Y-j_8tIH80Pn"}},{"cell_type":"code","source":["import jax, jax.numpy as jnp, jax.random as jr\n","import haiku as hk\n","import optax\n","from copy import deepcopy\n","import random\n","import networkx as nx\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","print(f\"Devices: {jax.devices()}\")"],"metadata":{"id":"EHZRU62P9B5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# @title The Iris Flowers Dataset\n","class IrisFlowers:\n","    def __init__(self, filename):\n","        self.file = filename\n","        df = pd.read_csv(self.file)\n","        dataset = df.to_dict(orient=\"list\")\n","        self.variety_to_index = {}\n","        self.index_to_variety = {}\n","\n","        for index, word in enumerate(set(dataset[\"variety\"])):\n","            self.variety_to_index[word] = index\n","            self.index_to_variety[index] = word\n","\n","        df[\"variety\"] = df[\"variety\"].map(self.variety_to_index)\n","\n","        df = df.sample(frac=1)\n","\n","        self.df = df\n","        self.df_np = self.df.to_numpy()\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        self.df_np[:, :4] = scaler.fit_transform(self.df_np[:, :4])\n","        self.length = len(df)\n","\n","    def single_sample(self) -> np.ndarray:\n","        random_selection = random.randint(0, self.length - 1)\n","        return self.df_np[random_selection]\n","\n","    def batched_sample(self, batch_size: int) -> np.ndarray:\n","        random_shuffled_df = np.copy(self.df_np)\n","        np.random.shuffle(random_shuffled_df)\n","        sample = random_shuffled_df[:batch_size]\n","        x, y = jnp.asarray(sample[:, :4]), jnp.asarray(sample[:, 4])\n","        y = y.astype(jnp.int32)\n","        return x, y\n","\n","dataset = IrisFlowers(\"./iris.csv\")"],"metadata":{"id":"E7IRO9pO_DMk","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Neural Network Creation and Mutation"],"metadata":{"id":"j548C-Fm9if_"}},{"cell_type":"code","source":["def create_graph(num_inputs, num_outputs):\n","    graph = nx.DiGraph()\n","    for i in range(1, num_inputs + 1):\n","        for o in range(num_inputs + 1, num_inputs + num_outputs + 1):\n","            graph.add_node(i, node=\"input\", weight=np.random.normal())\n","            if i == 1:\n","                graph.add_node(o, node=\"output\", weight=np.random.normal())\n","            graph.add_edge(i, o, enabled=True, weight=np.random.normal())\n","    return graph\n","\n","def vertex(graph):\n","    graph = nx.DiGraph(graph)\n","    i, o = random.choice(list(graph.edges))\n","    # graph.edges[(i, o)][\"enabled\"] = False\n","    new_node = max(graph.nodes) + 1\n","    graph.add_node(new_node, node=\"hidden\", weight=np.random.normal())\n","    graph.add_edge(i, new_node, enabled=True, weight=np.random.normal())\n","    graph.add_edge(new_node, o, enabled=True, weight=np.random.normal())\n","    return graph\n","\n","def connection(graph):\n","    graph = nx.DiGraph(graph)\n","    nodes = set(graph.nodes)\n","    inputs = {node for node in graph.nodes if graph.nodes[node][\"node\"] == \"input\"}\n","    outputs = {node for node in graph.nodes if graph.nodes[node][\"node\"] == \"output\"}\n","\n","    not_choosable = []\n","    while True:\n","        in_nodes = list(nodes - outputs - set(not_choosable))\n","        if len(in_nodes) == 0:\n","            return graph\n","        in_node = random.choice(in_nodes)\n","        out_nodes = nodes - inputs - {in_node} - set(nx.ancestors(graph, in_node))\n","        if len(out_nodes) == 0:\n","            not_choosable.append(in_node)\n","            continue\n","        out_node = random.choice(list(out_nodes))\n","        break\n","    if (in_node, out_node) in graph.edges:\n","        graph.edges[(in_node, out_node)][\"enabled\"] = True\n","    else:\n","        graph.add_edge(in_node, out_node, enabled=True, weight=np.random.normal())\n","    return graph\n","\n","def random_graph(inputs, outputs, mutations):\n","    graph = create_graph(inputs, outputs)\n","    mutate_probs = np.sin(np.arange(mutations)) / mutations\n","    for i in range(mutations):\n","        mutate_prob = np.sin(i) + 1\n","        if random.random() < mutate_prob:\n","            graph = vertex(graph)\n","        else:\n","            graph = connection(graph)\n","    return graph\n","\n","def plot_graph(graph, ax, with_weights=False):\n","    if not nx.is_directed_acyclic_graph(graph):\n","        return\n","    topological_order = list(nx.topological_sort(graph))\n","    pos = {}\n","    y = 0\n","    for generation in nx.topological_generations(graph):\n","        for x, node in enumerate(generation):\n","            if graph.nodes[node][\"node\"] == \"output\":\n","                continue\n","            pos[node] = (x, y)\n","        y += 1\n","    for x, node in enumerate([n for n in graph.nodes if graph.nodes[n][\"node\"] == \"output\"]):\n","        pos[node] = (x, y)\n","    edge_colors = [\"green\" if graph.edges[edge][\"enabled\"] else \"red\" for edge in graph.edges]\n","    nx.draw(graph, pos=pos, with_labels=True, ax=ax, edge_color=edge_colors)\n","    if with_weights:\n","        edge_labels = {edge: round(graph.edges[edge][\"weight\"], 2) for edge in graph.edges}\n","        nx.draw_networkx_edge_labels(graph, pos=pos, edge_labels=edge_labels, font_color=\"green\")\n","\n","graph = random_graph(4, 3, 5)"],"metadata":{"id":"bX7TeWNm9lW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SGD Algorithm"],"metadata":{"id":"yLwnuYUZ9fNU"}},{"cell_type":"code","source":["def get_params(graph):\n","    params = {\n","        **{f\"{node}\": jnp.array(graph.nodes[node][\"weight\"], dtype=jnp.float32) for node in graph.nodes},\n","        **{f\"{i}_{o}\": jnp.array(graph.edges[(i, o)][\"weight\"], dtype=jnp.float32) for i, o in graph.edges}\n","    }\n","    return hk.data_structures.to_immutable_dict(params)\n","\n","def set_params(graph, params):\n","    graph = nx.DiGraph(graph)\n","    for node in graph.nodes:\n","        graph.nodes[node][\"weight\"] = params[f\"{node}\"]\n","    for i, o in graph.edges:\n","        graph.edges[(i, o)][\"weight\"] = params[f\"{i}_{o}\"]\n","    return graph"],"metadata":{"id":"mNpmOPd3-WMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(graph, params, x):\n","    inputs = [node for node in graph.nodes if graph.nodes[node][\"node\"] == \"input\"]\n","    outputs = [node for node in graph.nodes if graph.nodes[node][\"node\"] == \"output\"]\n","    values = {i: x[:, idx] for idx, i in enumerate(inputs)}\n","    eval_stack, evaled_stack = [], []\n","\n","    for i in inputs:\n","        for s in graph.successors(i):\n","            if graph.edges[(i, s)][\"enabled\"] and s not in eval_stack:\n","                eval_stack.append(s)\n","\n","    while len(eval_stack) > 0:\n","        node = eval_stack.pop(0)\n","\n","        # Don't evaluate the node if it has already been evaluated.\n","        if node in values.keys():\n","            continue\n","\n","        # Get the enabled parents and the successors of the node.\n","        parents = {parent for parent in set(nx.all_neighbors(graph, node)) - set(graph.successors(node)) if graph.edges[(parent, node)][\"enabled\"]}\n","        successors = set(graph.successors(node)) - set(eval_stack)\n","\n","        if len(parents - set(values.keys())) == 0:\n","            value = 0\n","            for parent in parents:\n","                weight = params[f\"{parent}_{node}\"]\n","                value += weight * values[parent]\n","            value += params[f\"{node}\"]\n","            values[node] = jax.nn.relu(value)\n","            # Add successors that are not in the evaluation stack.\n","            eval_stack.extend(successors)\n","        else:\n","            eval_stack.extend(parents.union({node}) - set(eval_stack) - set(values.keys()))\n","\n","    output = jnp.stack([val for i, val in values.items() if i in outputs], axis=-1)\n","    output = jax.nn.softmax(output, axis=-1)\n","    return output"],"metadata":{"id":"njSnbWp5-XT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cross_entropy(y_pred, y, labels):\n","    eps = 1e-9\n","    y_pred = jnp.clip(y_pred, eps, 1 - eps)\n","    loss = -jnp.sum(y * jnp.log(y_pred), axis=-1)\n","    return jnp.mean(loss)\n","\n","@jax.value_and_grad\n","def loss_fn(params, graph, x, y):\n","    y_hat = evaluate(graph, params, x)\n","    return cross_entropy(y_hat, y, 3)\n","\n","def accuracy_fn(graph, params, x, y):\n","    y_hat = evaluate(graph, params, x)\n","    y_hat = jnp.argmax(y_hat, axis=-1)\n","    return jnp.mean(jnp.abs(y - y_hat) < 0.001)\n","\n","accuracy_fn = jax.jit(accuracy_fn, static_argnums=(0,))\n","\n","def step(graph, params, opt_state, x, y):\n","    loss, grads = loss_fn(params, graph, x, y)\n","    updates, opt_state = optimizer.update(grads, opt_state)\n","    params = optax.apply_updates(params, updates)\n","    return loss, params, opt_state\n","\n","step = jax.jit(step, static_argnums=(0,))"],"metadata":{"id":"t-66I5hy_Z1k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sanity check for the algorithm."],"metadata":{"id":"EXhuSWQK-2WL"}},{"cell_type":"code","source":["graph = random_graph(4, 3, 50)\n","params = get_params(graph)\n","optimizer = optax.adam(1e-4)\n","opt_state = optimizer.init(params)\n","\n","losses = []\n","accuracies = []\n","runtimes = []\n","\n","for _ in tqdm(range(5000)):\n","    x, y = dataset.batched_sample(64)\n","    loss, params, opt_state = step(graph, params, opt_state, x, jax.nn.one_hot(y, 3))\n","    accuracy = accuracy_fn(graph, params, x, y)\n","    losses.append(loss)\n","    accuracies.append(accuracy)\n","\n","\n","fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n","\n","ax1.plot(losses[::10])\n","ax2.plot(accuracies[::10])"],"metadata":{"id":"hZeYhcxI_Txq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evolutionary Aspect\n","\n","While there may be other algorithms for obtaining an optimal structure for neural networks. Evolution seems to make sense since the primary advantage of using evolution is its ability to explore unstructured spaces. In other words, it is difficult to predict the how adding a new node might affect the neural network as a whole."],"metadata":{"id":"5vo4m7N13mNc"}},{"cell_type":"code","source":["config = {\n","    # Evolution Parameters\n","    \"population_size\": 10,\n","    \"num_offsprings\": 10,\n","    \"vertex_mutation_prob\": 0.5,\n","    # SGD Parameters\n","    \"training_length\": 1000,\n","    \"learning_rate\": 1e-4,\n","    \"batch_size\": 64,\n","    \"optimizer\": optax.adam(1e-4)\n","}"],"metadata":{"id":"r0Wtpukz3oSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def learn(graph, config):\n","    params = get_params(graph)\n","    opt_state = config[\"optimizer\"].init(params)\n","    losses = []\n","\n","    for index in range(config[\"training_length\"]):\n","        x, y = dataset.batched_sample(config[\"batch_size\"])\n","        y = jax.nn.one_hot(y, 3)\n","        loss, params, opt_state = step(graph, params, opt_state, x, y)\n","        losses.append(loss)\n","\n","    graph = set_params(graph, params)\n","    return np.array(losses), graph\n","\n","graph = create_graph(4, 3)\n","losses, graph = learn(graph, config)"],"metadata":{"id":"dFKgW9Ks4_bh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the initial population. Since the population has the same nodes. It makes sense to only train one model and duplicate it for the entire population. Optimally, the nodes should converge at the same weights either way."],"metadata":{"id":"V7eRFoV-4s_x"}},{"cell_type":"code","source":["graph = create_graph(4, 3)\n","losses, graph = learn(graph, config)\n","population = [deepcopy(graph) for _ in range(config['population_size'])]"],"metadata":{"id":"F65q8RcF44VE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each generation, calculate a new population that is the best of the mutated individuals and the previous generation. Then train the combined generation."],"metadata":{"id":"C0GIVzuk4p6b"}},{"cell_type":"code","source":["offsprings = []\n","for _ in range(config['num_offsprings']):\n","    random_individual = random.choice(population)\n","    offspring = deepcopy(random_individual)\n","    offspring = vertex(offspring) if random.random() < config['vertex_mutation_prob'] else connection(offspring)\n","    offsprings.append(offspring)\n","\n","combined_population = population + offsprings"],"metadata":{"id":"YNKQ_3EaBH5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_losses = []\n","for i in tqdm(range(len(combined_population))):\n","    loss, offspring = learn(combined_population[i], config)\n","    combined_population[i] = offspring\n","    combined_losses.append(loss)"],"metadata":{"id":"OoW1UjhRC-1C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now remove the worst individuals using the fitness score, in this case accuracy."],"metadata":{"id":"rCtd_-WFFm34"}},{"cell_type":"code","source":["fitness_scores = []\n","for individual in combined_population:\n","    params = get_params(individual)\n","    x, y = dataset.batched_sample(128)\n","    accuracy = accuracy_fn(individual, params, x, y)\n","    fitness_scores.append(accuracy)\n","\n","indices = np.flip(np.argsort(fitness_scores))"],"metadata":{"id":"M2aR34W2FuYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_population = [combined_population[i] for i in indices[:config['population_size']]]\n","new_fitness_scores = [fitness_scores[i] for i in indices[:config['population_size']]]"],"metadata":{"id":"Xq2ne3oSIO5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Complete Algorithm\n","\n","The below code runs over the given number of generations."],"metadata":{"id":"goQyJD7aJ8Bn"}},{"cell_type":"code","source":["config = {\n","    # Evolution Parameters\n","    \"population_size\": 10,\n","    \"num_offsprings\": 10,\n","    \"vertex_mutation_prob\": 0.5,\n","    \"num_generations\": 30,\n","    # SGD Parameters\n","    \"training_length\": 200,\n","    \"learning_rate\": 1e-4,\n","    \"batch_size\": 64,\n","    \"optimizer\": optax.adam(1e-4)\n","}\n","\n","def learn(graph, config):\n","    params = get_params(graph)\n","    opt_state = config[\"optimizer\"].init(params)\n","    losses = []\n","\n","    for index in range(config[\"training_length\"]):\n","        x, y = dataset.batched_sample(config[\"batch_size\"])\n","        y = jax.nn.one_hot(y, 3)\n","        loss, params, opt_state = step(graph, params, opt_state, x, y)\n","        losses.append(loss)\n","\n","    graph = set_params(graph, params)\n","    return np.array(losses), graph\n","\n","# Initial Population\n","graph = create_graph(4, 3)\n","losses, graph = learn(graph, config)\n","population = [deepcopy(graph) for _ in range(config['population_size'])]\n","\n","for generation in range(config[\"num_generations\"]):\n","    print(f\"Generation: {generation + 1} / {config['num_generations']}\")\n","    offsprings = []\n","    for _ in range(config['num_offsprings']):\n","        random_individual = random.choice(population)\n","        offspring = deepcopy(random_individual)\n","        offspring = vertex(offspring) if random.random() < config['vertex_mutation_prob'] else connection(offspring)\n","        offsprings.append(offspring)\n","\n","    combined_population = population + offsprings\n","\n","    # Train the new population.\n","    for i in tqdm(range(len(combined_population))):\n","        loss, offspring = learn(combined_population[i], config)\n","        combined_population[i] = offspring\n","\n","    fitness_scores = []\n","    for individual in combined_population:\n","        params = get_params(individual)\n","        x, y = dataset.batched_sample(128)\n","        accuracy = accuracy_fn(individual, params, x, y)\n","        fitness_scores.append(accuracy)\n","\n","    indices = np.flip(np.argsort(fitness_scores))\n","\n","    population = [combined_population[i] for i in indices[:config['population_size']]]\n","    fitness_scores = [fitness_scores[i] for i in indices[:config['population_size']]]\n","\n","    print(f\"Min: {np.min(fitness_scores)}\\tMean: {np.mean(fitness_scores)}\\tMax: {np.max(fitness_scores)}\")"],"metadata":{"id":"f_hfQ_lLknSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ulabul6SkwkB"},"execution_count":null,"outputs":[]}]}